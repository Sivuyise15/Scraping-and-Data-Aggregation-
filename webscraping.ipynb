{
  "metadata": {
    "kernelspec": {
      "name": "xpython",
      "display_name": "Python 3.13 (XPython)",
      "language": "python"
    },
    "language_info": {
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "version": "3.13.1"
    }
  },
  "nbformat_minor": 5,
  "nbformat": 4,
  "cells": [
    {
      "id": "2530c707-ae02-493c-afd9-8b6ffbc6c25a",
      "cell_type": "markdown",
      "source": "# Scraping and Data Aggregation - Reviews From a Public Website",
      "metadata": {}
    },
    {
      "id": "8eb46713-0932-48d7-8d6c-798078e81bc1",
      "cell_type": "markdown",
      "source": "Firstly, installing python libraries that I will be using ",
      "metadata": {}
    },
    {
      "id": "8d1512ee-2a38-494e-bc57-391dc2fce318",
      "cell_type": "code",
      "source": "%pip install requests BeautifulSoup4",
      "metadata": {
        "trusted": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": "Process pip requirements ...\n\nRequirement typing-extensions already satisfied.\nSuccessfully installed requests-2.32.4 charset_normalizer-3.4.2 idna-3.10 urllib3-2.5.0 certifi-2025.6.15 BeautifulSoup4-4.13.4 soupsieve-2.7\n"
        }
      ],
      "execution_count": 1
    },
    {
      "id": "0b347f74-4e34-4f25-b0ae-755732ff5688",
      "cell_type": "markdown",
      "source": "Now importing the libraries that I will be using",
      "metadata": {}
    },
    {
      "id": "b75661c1-9b69-4950-92f3-1dc429423d49",
      "cell_type": "code",
      "source": "import requests\nfrom bs4 import BeautifulSoup\nimport csv",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": 2
    },
    {
      "id": "ced0d1c3-d2a1-4b4a-870e-04b9559e13c3",
      "cell_type": "code",
      "source": "# getting the url of the website to scrape\nurl = input(\"Enter the website URL to scrape:\")",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": 3
    },
    {
      "id": "8985d326-9735-4c67-8aab-f987bea13bf5",
      "cell_type": "markdown",
      "source": "Assuming the container name of the reviews is \"review\"",
      "metadata": {}
    },
    {
      "id": "6a3bdf2a-ab5b-4469-9dd9-bed335c33e9d",
      "cell_type": "markdown",
      "source": "Opening the CSV file to write to and writing to it, the scraped reviews data, I am assuming that the reviews html text looks like something like this:\n```html\n<div class=\"review\" id=\"review-101\">\n    <span class=\"author\">Sivuyise Matwa</span>\n    <span class=\"date\">April 225</span>\n    <span class=\"rating\">5</span>\n    <p class=\"text\">The product works fine</p>\n</div>\n```",
      "metadata": {}
    },
    {
      "id": "746875b5-be84-42ee-8545-d4fb7dcd0ad0",
      "cell_type": "code",
      "source": "try:\n    response = requests.get(url)\n    soup = BeautifulSoup(response.text, 'html.parser')\n\n    reviews = soup.find_all('div', class_='review') # The reviews in the html file\n    with open('Reviews.csv', 'w', newline='', encoding='utf-8') as csvfile:\n        writer = csv.writer(csvfile)\n        # Header in the csv file\n        writer.writerow(['review_id', 'author', 'date', 'rating', 'text'])\n\n        for review in reviews:\n            review_id = review.get('id', 'N/A')\n            author = review.find('span', class_='author').get_text(strip=True)\n            date = review.find('span', class_='date').get_text(strip=True)\n            rating = review.find('span', class_='rating').get_text(strip=True)\n            text = review.find('p', class_='text').get_text(strip=True)\n\n            print(f\"{review_id}  {author}  {date}  {rating}  {text}\")\n            writer.writerow([review_id, author, date, rating, text])\n    print(\"Reviews written to a csv file\")\n\nexcept Exception as e:\n    print(\"Exception occurred, the website might not be allowing scraping\")",
      "metadata": {
        "trusted": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": "Exception occurred, the website might not be allowing scraping\n"
        }
      ],
      "execution_count": 4
    },
    {
      "id": "fb6b6a39-7ebe-4d9a-b28c-e9bc555ba910",
      "cell_type": "markdown",
      "source": "### Assumptions\nOne of the most important assumptions I made was the structure of the container of the reviewss in the html file, I have pasted the example of the structure that the current code would accept above. I have tested in with this website: https://sivuyise15.github.io/my-website/  where I have the exact structure as decribed above.\n\n### Approach\nI firstly installed the libraries I neeeded. I used the requests library to fetch the HTML content of the web page I want to scrape and then folllowed BeautifulSoup library to parse and navigate the HTML content after fetching it. Lastly, I used the csv library to open and write to the csv file. \n\n### Dev opportunities and Edge case\nthe current code only conforms to one reviews structure, which is the one I have above, it can be extended to also accept other websites that have different structure the reveiws so we can scrape as much as we need withoute limitations of html file review structure. I tried to make it crash peacefully by using the try and except block, incase errors arrises when fetching the html file. Edge case like wrong url, missing fields, connection errors can also be handled well and in a good coding manner.\n",
      "metadata": {}
    },
    {
      "id": "633823a9-177a-4a99-8243-57d38a7c9bee",
      "cell_type": "code",
      "source": "",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    }
  ]
}